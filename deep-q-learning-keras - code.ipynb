{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Q-Learning with Keras and Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=VERBOSE)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # The essential Q-learning update...\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state, verbose=VERBOSE)[0]))\n",
    "                \n",
    "            target_f = self.model.predict(state, verbose=VERBOSE)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # in this case we do a one-by-one update\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=VERBOSE)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQNBatchAgent(DQNAgent):\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, targets_f = [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # The essential Q-learning update...\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state, verbose=VERBOSE)[0]))\n",
    "                \n",
    "            target_f = self.model.predict(state, verbose=VERBOSE)\n",
    "            target_f[0][action] = target \n",
    "            \n",
    "            #----------------------------------------------\n",
    "            # Filtering out states and targets for training\n",
    "            states.append(state[0])\n",
    "            targets_f.append(target_f[0])\n",
    "            #----------------------------------------------\n",
    "        \n",
    "        # in this case we do a batch update    \n",
    "        history = self.model.fit(np.array(states), np.array(targets_f), epochs=1, verbose=VERBOSE)\n",
    "        \n",
    "        # Keeping track of loss\n",
    "        loss = history.history['loss'][0]\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__(state_size, action_size)\n",
    "        # We additionally create a second \"target_model\"...\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss, # <-- this is the only difference with the DNQ model\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model        \n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\n",
    "    \"\"\"\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state, verbose=VERBOSE)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state, verbose=VERBOSE)[0]\n",
    "                # The essential Q-learning update... but \"t\" comes from the \"target_model\"!\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "                \n",
    "            # in this case we do a one-by-one update\n",
    "            self.model.fit(state, target, epochs=1, verbose=VERBOSE)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5, score: 22, e: 1.0\n",
      "episode: 1/5, score: 62, e: 0.77\n",
      "episode: 2/5, score: 21, e: 0.69\n",
      "episode: 3/5, score: 15, e: 0.64\n",
      "episode: 4/5, score: 165, e: 0.28\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "MODEL_TYPE = 'DQN' # 'DDQN' 'DQNBatch'\n",
    "MODEL_TYPE = 'DQNBatch'\n",
    "MODEL_TYPE = 'DDQN'\n",
    "\n",
    "if MODEL_TYPE == 'DQN':        agent = DQNAgent(state_size, action_size)\n",
    "elif MODEL_TYPE == 'DDQN':     agent = DDQNAgent(state_size, action_size)\n",
    "elif MODEL_TYPE == 'DQNBatch': agent = DQNBatchAgent(state_size, action_size)\n",
    "\n",
    "# agent.load(f\"saved_models/cartpole-{MODEL_TYPE}.h5\")\n",
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "VERBOSE = 0\n",
    "\n",
    "EPISODES = 5 # 5_000\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state,info = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if MODEL_TYPE in ['DQN','DQNBatch']:\n",
    "            reward = reward if not done else -10\n",
    "        elif MODEL_TYPE == 'DDQN':\n",
    "            x,x_dot,theta,theta_dot = next_state\n",
    "            r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "            r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "            reward = r1 + r2\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            if MODEL_TYPE == 'DDQN':\n",
    "                agent.update_target_model() # other \"cyles\" for this update could also be possible\n",
    "                \n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            loss = agent.replay(batch_size)\n",
    "            if MODEL_TYPE == 'DQNBatch':\n",
    "                # Logging training loss every 10 timesteps\n",
    "                if time % 10 == 0:\n",
    "                    print(\"-- episode: {}/{}, time: {}, loss: {:.4f}\"\n",
    "                        .format(e, EPISODES, time, loss))  \n",
    "            \n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(f\"saved_models/cartpole-{MODEL_TYPE}.h5\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
